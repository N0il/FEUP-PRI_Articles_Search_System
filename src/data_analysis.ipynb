{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARXIV Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': 'http://arxiv.org/abs/1802.00209v1', 'summary': 'We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.', 'title': 'Dual Recurrent Attention Units for Visual Question Answering', 'authors': ['Ahmed Osman', 'Wojciech Samek'], 'date': '2018-2-1', 'tags': ['cs.AI', 'cs.CL', 'cs.CV', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1603.03827v1', 'summary': 'Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.', 'title': 'Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks', 'authors': ['Ji Young Lee', 'Franck Dernoncourt'], 'date': '2016-3-12', 'tags': ['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1606.00776v2', 'summary': 'We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.', 'title': 'Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation', 'authors': ['Iulian Vlad Serban', 'Tim Klinger', 'Gerald Tesauro', 'Kartik Talamadupula', 'Bowen Zhou', 'Yoshua Bengio', 'Aaron Courville'], 'date': '2016-6-2', 'tags': ['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1705.08142v2', 'summary': 'Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.', 'title': 'Learning what to share between loosely related tasks', 'authors': ['Sebastian Ruder', 'Joachim Bingel', 'Isabelle Augenstein', 'Anders SÃ¸gaard'], 'date': '2017-5-23', 'tags': ['stat.ML', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.NE']}\n",
      "{'link': 'http://arxiv.org/abs/1709.02349v2', 'summary': 'We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.', 'title': 'A Deep Reinforcement Learning Chatbot', 'authors': ['Iulian V. Serban', 'Chinnadhurai Sankar', 'Mathieu Germain', 'Saizheng Zhang', 'Zhouhan Lin', 'Sandeep Subramanian', 'Taesup Kim', 'Michael Pieper', 'Sarath Chandar', 'Nan Rosemary Ke', 'Sai Rajeshwar', 'Alexandre de Brebisson', 'Jose M. R. Sotelo', 'Dendi Suhubdy', 'Vincent Michalski', 'Alexandre Nguyen', 'Joelle Pineau', 'Yoshua Bengio'], 'date': '2017-9-7', 'tags': ['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1709.08878v1', 'summary': 'We propose a new generative model of sentences that first samples a prototype\\nsentence from the training corpus and then edits it into a new sentence.\\nCompared to traditional models that generate from scratch either left-to-right\\nor by first sampling a latent sentence vector, our prototype-then-edit model\\nimproves perplexity on language modeling and generates higher quality outputs\\naccording to human evaluation. Furthermore, the model gives rise to a latent\\nedit vector that captures interpretable semantics such as sentence similarity\\nand sentence-level analogies.', 'title': 'Generating Sentences by Editing Prototypes', 'authors': ['Kelvin Guu', 'Tatsunori B. Hashimoto', 'Yonatan Oren', 'Percy Liang'], 'date': '2017-9-26', 'tags': ['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1801.06700v1', 'summary': 'We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including neural network and\\ntemplate-based models. By applying reinforcement learning to crowdsourced data\\nand real-world user interactions, the system has been trained to select an\\nappropriate response from the models in its ensemble. The system has been\\nevaluated through A/B testing with real-world users, where it performed\\nsignificantly better than other systems. The results highlight the potential of\\ncoupling ensemble systems with deep reinforcement learning as a fruitful path\\nfor developing real-world, open-domain conversational agents.', 'title': 'A Deep Reinforcement Learning Chatbot (Short Version)', 'authors': ['Iulian V. Serban', 'Chinnadhurai Sankar', 'Mathieu Germain', 'Saizheng Zhang', 'Zhouhan Lin', 'Sandeep Subramanian', 'Taesup Kim', 'Michael Pieper', 'Sarath Chandar', 'Nan Rosemary Ke', 'Sai Rajeswar', 'Alexandre de Brebisson', 'Jose M. R. Sotelo', 'Dendi Suhubdy', 'Vincent Michalski', 'Alexandre Nguyen', 'Joelle Pineau', 'Yoshua Bengio'], 'date': '2018-1-20', 'tags': ['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']}\n",
      "{'link': 'http://arxiv.org/abs/1609.06492v1', 'summary': 'The paper introduces a new method for discrimination of documents given in\\ndifferent scripts. The document is mapped into a uniformly coded text of\\nnumerical values. It is derived from the position of the letters in the text\\nline, based on their typographical characteristics. Each code is considered as\\na gray level. Accordingly, the coded text determines a 1-D image, on which\\ntexture analysis by run-length statistics and local binary pattern is\\nperformed. It defines feature vectors representing the script content of the\\ndocument. A modified clustering approach employed on document feature vector\\ngroups documents written in the same script. Experimentation performed on two\\ncustom oriented databases of historical documents in old Cyrillic, angular and\\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\\nsuperiority of the proposed method with respect to well-known methods in the\\nstate-of-the-art.', 'title': 'Document Image Coding and Clustering for Script Discrimination', 'authors': ['Darko Brodic', 'Alessia Amelio', 'Zoran N. Milivojevic', 'Milena Jevtic'], 'date': '2016-9-21', 'tags': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.NE']}\n",
      "{'link': 'http://arxiv.org/abs/1610.01076v1', 'summary': 'Together with the development of more accurate methods in Computer Vision and\\nNatural Language Understanding, holistic architectures that answer on questions\\nabout the content of real-world images have emerged. In this tutorial, we build\\na neural-based approach to answer questions about images. We base our tutorial\\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\\nmodels that we present here can achieve a competitive performance on both\\ndatasets, in fact, they are among the best methods that use a combination of\\nLSTM with a global, full frame CNN representation of an image. We hope that\\nafter reading this tutorial, the reader will be able to use Deep Learning\\nframeworks, such as Keras and introduced Kraino, to build various architectures\\nthat will lead to a further performance improvement on this challenging task.', 'title': 'Tutorial on Answering Questions about Images with Deep Learning', 'authors': ['Mateusz Malinowski', 'Mario Fritz'], 'date': '2016-10-4', 'tags': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG', 'cs.NE']}\n",
      "{'link': 'http://arxiv.org/abs/1705.07962v2', 'summary': 'Transforming a graphical user interface screenshot created by a designer into\\ncomputer code is a typical task conducted by a developer in order to build\\ncustomized software, websites, and mobile applications. In this paper, we show\\nthat deep learning methods can be leveraged to train a model end-to-end to\\nautomatically generate code from a single input image with over 77% of accuracy\\nfor three different platforms (i.e. iOS, Android and web-based technologies).', 'title': 'pix2code: Generating Code from a Graphical User Interface Screenshot', 'authors': ['Tony Beltramelli'], 'date': '2017-5-22', 'tags': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.NE']}\n"
     ]
    }
   ],
   "source": [
    "#Clean dataset\n",
    "\n",
    "def contains_number(string):\n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "with open('./data/arxivData.json', 'r') as dataset:\n",
    "    papers = json.loads(dataset.read())\n",
    "\n",
    "for paper in papers:\n",
    "    names = []\n",
    "\n",
    "    #authors\n",
    "    for author in ast.literal_eval(paper['author']):        \n",
    "        names.append(author['name'])\n",
    "    paper['authors'] = names\n",
    "\n",
    "    #link\n",
    "    for link in ast.literal_eval(paper['link']):\n",
    "        if link['rel'] == 'alternate':\n",
    "            paper['link'] = link['href']\n",
    "\n",
    "    #date\n",
    "    date = str(paper['year']) + '-' + str(paper['month']) + '-' +  str(paper['day']) \n",
    "    paper['date'] = date\n",
    "\n",
    "    #tags\n",
    "    tags = []\n",
    "    for tag in ast.literal_eval(paper['tag']):\n",
    "        if (contains_number(tag['term'])):\n",
    "            continue\n",
    "        tags.append(tag['term'])\n",
    "    paper['tags'] = tags\n",
    "\n",
    "    paper.pop('author')\n",
    "    paper.pop('id')\n",
    "    paper.pop('year')\n",
    "    paper.pop('month')\n",
    "    paper.pop('day')\n",
    "    paper.pop('tag')\n",
    "\n",
    "\n",
    "for x in range(10):\n",
    "    print(papers[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = ['link', 'summary', 'title', 'authors', 'date', 'tags']\n",
    "with open('dateset.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'artificial intelligence ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'computation and language ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'computer vision and pattern recognition ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'neural and evolutionary computing ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'machine learning ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'physics and society ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'applications ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'robotics ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'software engineering ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'multiagent systems ' exist\n",
      "---calling getTitle---\n",
      "No listing 'pastweek' for 'optimization and control ' exist\n",
      "---calling getTitle---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m urlTags:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tagsDict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 28\u001b[0m         tagsDict[tag] \u001b[38;5;241m=\u001b[39m getTitle(tag)\n\u001b[1;32m     30\u001b[0m     newTags\u001b[38;5;241m.\u001b[39mappend(tagsDict[tag])\n\u001b[1;32m     31\u001b[0m paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m newTags\n",
      "Cell \u001b[0;32mIn [36], line 8\u001b[0m, in \u001b[0;36mgetTitle\u001b[0;34m(tag)\u001b[0m\n\u001b[1;32m      5\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org/list/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m URL \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/recent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     11\u001b[0m dlPageIndex \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlpage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py:76\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    537\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    538\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    539\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    540\u001b[0m }\n\u001b[1;32m    541\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 542\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    654\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    657\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    658\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    440\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    441\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    442\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    443\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    444\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    445\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    449\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:382\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    384\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1012\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1015\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1016\u001b[0m         (\n\u001b[1;32m   1017\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1023\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:411\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    403\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    404\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    408\u001b[0m ):\n\u001b[1;32m    409\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    412\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    413\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    414\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    415\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    416\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    417\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    418\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    419\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    420\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    421\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    422\u001b[0m )\n\u001b[1;32m    424\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    428\u001b[0m     default_ssl_context\n\u001b[1;32m    429\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    432\u001b[0m ):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    508\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1038\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1041\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def getTitle(tag):\n",
    "    print(\"---calling getTitle---\")\n",
    "    URL = 'https://arxiv.org/list/'\n",
    "    url = URL + tag + '/recent'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "\n",
    "    dlPageIndex = text.find(\"dlpage\")\n",
    "\n",
    "    h1Index = text.find(\"<h1>\", dlPageIndex+1)\n",
    "    h1FinalIndex = text.find(\"</h1>\", h1Index)\n",
    "\n",
    "    result = text[h1Index+4:h1FinalIndex]\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "tagsDict = {}\n",
    "\n",
    "for paper in papers:\n",
    "    urlTags = paper['tags']\n",
    "    newTags = []\n",
    "    \n",
    "    for tag in urlTags:\n",
    "        if tag not in tagsDict.keys():\n",
    "            tagsDict[tag] = getTitle(tag)\n",
    "        \n",
    "        newTags.append(tagsDict[tag])\n",
    "    paper['tags'] = newTags\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(papers[i])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
